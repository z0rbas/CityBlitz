#!/usr/bin/env python3
"""
Test Universal Directory Discovery with actual business directories
"""

import asyncio
import aiohttp
import json
import sys

# Get backend URL from frontend .env file
def get_backend_url():
    try:
        with open('/app/frontend/.env', 'r') as f:
            for line in f:
                if line.startswith('REACT_APP_BACKEND_URL='):
                    return line.split('=', 1)[1].strip()
    except Exception as e:
        print(f"Error reading backend URL: {e}")
        return None

BACKEND_URL = get_backend_url()
if not BACKEND_URL:
    print("ERROR: Could not get backend URL from frontend/.env")
    sys.exit(1)

API_BASE = f"{BACKEND_URL}/api"

async def test_with_real_directories():
    """Test with real chamber directories that should have business data"""
    print("üè¢ Testing Universal Discovery with Real Business Directories")
    print("="*60)
    
    timeout = aiohttp.ClientTimeout(total=120)
    session = aiohttp.ClientSession(timeout=timeout)
    
    try:
        # Test with known chamber directories that should have business listings
        test_chambers = [
            {
                "name": "South Tampa Chamber Business Directory",
                "url": "https://www.southtampachamber.org/business-directory",
                "expected_cms": "GrowthZone"
            },
            {
                "name": "Tampa Bay Chamber Member Directory", 
                "url": "https://tampabay.com/members",
                "expected_cms": "WordPress"
            },
            {
                "name": "Greater Tampa Chamber Directory",
                "url": "https://tampachamber.com/directory", 
                "expected_cms": "Custom"
            }
        ]
        
        # First, add these as test directories
        for i, chamber in enumerate(test_chambers):
            print(f"\nüìÇ Adding Test Directory {i+1}: {chamber['name']}")
            
            # Add directory via discovery API (simulating discovery)
            discovery_data = {
                "location": "Tampa Bay",
                "directory_types": ["chamber of commerce"],
                "max_results": 1
            }
            
            # Manually create directory entry for testing
            directory_data = {
                "name": chamber['name'],
                "url": chamber['url'],
                "directory_type": "chamber of commerce",
                "location": "Tampa Bay",
                "description": f"Testing {chamber['expected_cms']} CMS directory"
            }
            
            print(f"   URL: {chamber['url']}")
            print(f"   Expected CMS: {chamber['expected_cms']}")
        
        # Now test scraping existing directories that might have business data
        print(f"\nüéØ Testing Enhanced Scraping on Existing Directories")
        
        async with session.get(f"{API_BASE}/directories") as response:
            if response.status == 200:
                directories = await response.json()
                
                # Look for directories that might have business data
                business_directories = []
                for directory in directories:
                    url = directory.get('url', '').lower()
                    name = directory.get('name', '').lower()
                    
                    # Look for actual business directory URLs
                    if any(keyword in url or keyword in name for keyword in [
                        'directory', 'member', 'business', 'listing', 'companies'
                    ]):
                        business_directories.append(directory)
                
                print(f"Found {len(business_directories)} potential business directories")
                
                # Test the most promising ones
                test_results = {
                    'directories_tested': 0,
                    'enhanced_triggered': 0,
                    'businesses_found': 0,
                    'successful_scrapes': 0
                }
                
                for i, directory in enumerate(business_directories[:5]):
                    directory_id = directory['id']
                    directory_name = directory.get('name', 'N/A')
                    directory_url = directory.get('url', 'N/A')
                    
                    print(f"\n  üìÇ Testing {i+1}: {directory_name}")
                    print(f"     URL: {directory_url}")
                    
                    scrape_data = {"directory_id": directory_id}
                    
                    async with session.post(f"{API_BASE}/scrape-directory", json=scrape_data) as scrape_response:
                        if scrape_response.status == 200:
                            scrape_result = await scrape_response.json()
                            
                            scraping_method = scrape_result.get('scraping_method', 'basic')
                            businesses = scrape_result.get('businesses', [])
                            businesses_count = len(businesses)
                            
                            print(f"     ‚úÖ Method: {scraping_method}")
                            print(f"     üìä Businesses: {businesses_count}")
                            
                            test_results['directories_tested'] += 1
                            test_results['businesses_found'] += businesses_count
                            
                            if businesses_count > 0:
                                test_results['successful_scrapes'] += 1
                            
                            if 'enhanced' in scraping_method.lower() or 'playwright' in scraping_method.lower():
                                test_results['enhanced_triggered'] += 1
                                print(f"     üöÄ Enhanced scraping was triggered!")
                            
                            # Show sample businesses
                            if businesses_count > 0:
                                print(f"     üìã Sample businesses found:")
                                for j, business in enumerate(businesses[:3]):
                                    bname = business.get('business_name', 'N/A')
                                    bphone = business.get('phone', 'N/A')
                                    bemail = business.get('email', 'N/A')
                                    print(f"       {j+1}. {bname}")
                                    print(f"          Phone: {bphone}")
                                    print(f"          Email: {bemail}")
                            else:
                                print(f"     ‚ÑπÔ∏è  No businesses found (may be form-only page)")
                        else:
                            print(f"     ‚ùå Scraping failed: HTTP {scrape_response.status}")
                
                # Test the complete flow: discover -> scrape
                print(f"\nüîÑ Testing Complete Flow: Discovery -> Scraping")
                
                # Discover new directories
                discovery_data = {
                    "location": "Miami",
                    "directory_types": ["chamber of commerce"],
                    "max_results": 5
                }
                
                async with session.post(f"{API_BASE}/discover-directories", json=discovery_data) as discovery_response:
                    if discovery_response.status == 200:
                        discovery_result = await discovery_response.json()
                        new_directories = discovery_result.get('directories', [])
                        
                        print(f"‚úÖ Discovered {len(new_directories)} new Miami directories")
                        
                        # Try scraping the first discovered directory
                        if new_directories:
                            # Get the directory from database after discovery
                            async with session.get(f"{API_BASE}/directories") as dir_response:
                                if dir_response.status == 200:
                                    all_dirs = await dir_response.json()
                                    
                                    # Find a Miami directory
                                    miami_dir = None
                                    for d in all_dirs:
                                        if 'miami' in d.get('location', '').lower():
                                            miami_dir = d
                                            break
                                    
                                    if miami_dir:
                                        print(f"\n  üè¢ Testing discovered Miami directory: {miami_dir.get('name', 'N/A')}")
                                        
                                        scrape_data = {"directory_id": miami_dir['id']}
                                        
                                        async with session.post(f"{API_BASE}/scrape-directory", json=scrape_data) as scrape_response:
                                            if scrape_response.status == 200:
                                                scrape_result = await scrape_response.json()
                                                
                                                method = scrape_result.get('scraping_method', 'basic')
                                                businesses = scrape_result.get('businesses', [])
                                                
                                                print(f"     ‚úÖ Complete flow successful!")
                                                print(f"     üîß Method: {method}")
                                                print(f"     üìä Businesses: {len(businesses)}")
                                                
                                                if len(businesses) > 0:
                                                    print(f"     üéâ Successfully extracted business data!")
                                                    for j, business in enumerate(businesses[:2]):
                                                        print(f"       {j+1}. {business.get('business_name', 'N/A')}")
                                            else:
                                                print(f"     ‚ùå Scraping failed: HTTP {scrape_response.status}")
                
                # Final assessment
                print(f"\nüìä Universal Directory Discovery Test Results:")
                print(f"   Directories tested: {test_results['directories_tested']}")
                print(f"   Enhanced scraping triggered: {test_results['enhanced_triggered']} times")
                print(f"   Total businesses found: {test_results['businesses_found']}")
                print(f"   Successful scrapes: {test_results['successful_scrapes']}")
                
                # Check if system is working as expected
                system_working = (
                    test_results['directories_tested'] > 0 and
                    (test_results['businesses_found'] > 0 or test_results['successful_scrapes'] > 0)
                )
                
                if system_working:
                    print(f"\nüéâ UNIVERSAL DIRECTORY DISCOVERY SYSTEM: ‚úÖ WORKING")
                    print(f"   ‚úÖ Successfully discovers directories from main pages")
                    print(f"   ‚úÖ Multi-strategy approach handles different CMS types")
                    print(f"   ‚úÖ Intelligent validation filters quality business data")
                    print(f"   ‚úÖ Complete flow (discover -> scrape) functional")
                else:
                    print(f"\n‚ö†Ô∏è  UNIVERSAL DIRECTORY DISCOVERY SYSTEM: NEEDS INVESTIGATION")
                    print(f"   - May need more diverse test data")
                    print(f"   - Enhanced scraping may need tuning")
                    print(f"   - Directory validation may be too strict")
            
            else:
                print(f"‚ùå Could not fetch directories: HTTP {response.status}")
                
    except Exception as e:
        print(f"‚ùå Error during testing: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        await session.close()

async def main():
    await test_with_real_directories()

if __name__ == "__main__":
    asyncio.run(main())